## ğŸ•·ï¸ Spidey Sense
**Navigate the world, fearlessly.**<br>

AI-powered navigation assistant that helps visually impaired users explore their surroundings through real-time detection, spatial awareness, and conversational guidance.
Watch Here: (https://youtu.be/SmBhXeC8LCg?si=urIZ8hoMH9UmX-Rm)<br>

---
## ğŸŒ Social Impact
More than **285 million** people worldwide live with visual impairments. Traditional canes and GPS apps offer **limited awareness** â€” they canâ€™t describe nearby obstacles or open paths.<br>

Spidey Sense transforms independence by **turning vision into voice and touch**, guiding users with real-time spatial awareness, conversational AI, and natural speech â€” empowering safer, more confident mobility.

---
## ğŸ§  Inspiration
We asked: â€œWhat if someone who canâ€™t see could still **sense** the world like Spider-Man?â€ <br>

Most navigation tools tell you **where you are**, not **whatâ€™s around you**.
Visually impaired users often wonder:
> *â€œIs there something in front of me?â€<br>
> â€œCan I move forward safely?â€*<br>

So we built **Spidey Sense** â€” a friendly AI companion that *sees, thinks, and speaks*, helping users explore with awareness and trust.<br>

---
## ğŸ’¡ What It Does
ğŸ§  **Real-Time Object Detection** â€” Detects people, chairs, doors, and obstacles using **COCO-SSD**.<br>
ğŸ¦¯ **Spatial Awareness Engine** â€” Classifies objects into left, center, right zones for precise guidance.<br>
ğŸ—£ï¸ **Conversational Companion (OpenAI API)** â€” Users ask questions like *â€œWhatâ€™s around me?â€* or *â€œWhere can I go?â€* and get natural, scene-aware responses.<br>
ğŸ”Š **Voice Synthesis (ElevenLabs)** â€” Converts Geminiâ€™s replies into lifelike speech.<br>
ğŸ¥ **Smart Timer** â€” Periodically checks surroundings every 10 seconds or when voice is triggered.<br>
ğŸª„ **Multi-Mode Awareness** â€” Switch between **Explore**, **Focus**, and **Follow** for different contexts.<br>

---

## ğŸŒŸ Key Benefits

ğŸ‘ï¸ **Vision â†’ Voice** â€” Narrates your environment in real time.<br>
ğŸ¦¯ **Safe Movement** â€” Guides you away from dead ends and toward clear paths.<br>
ğŸ§  **Conversational Insight** â€” Natural dialogue, not robotic alerts.<br>
ğŸ¤± **Touch Expansion** â€” Future-ready for haptic feedback integration.<br>
ğŸŒ **Accessibility First** â€” Voice-first, minimalistic design built for independence.<br>

---

## ğŸš€ Use Cases

* Pedestrian navigation for the visually impaired
* Campus or indoor mobility for students
* Elderly users navigating homes and care facilities
* Assistive tech developers integrating multimodal AI

---

## ğŸ› ï¸ How We Built It

### ğŸ”¥ Frontend

* React, CSS, JavaScript for a **voice-first UI**
* Web Speech API for push-to-talk and voice capture
* Mock interface simulating object detection + Gemini dialogue

### âš™ï¸ Backend

* **Node.js + Express** for API routing
* **COCO-SSD** model for live object detection
* **Open AI API** for contextual understanding and natural responses
* **ElevenLabs API** for lifelike voice output

### ğŸ¤– AI & APIs

* **OpenAI API** â†’ conversational reasoning and scene description
* **ElevenLabs TTS** â†’ natural speech output
* **COCO-SSD** â†’ real-time detection for 80+ object classes

---

## ğŸ›‡ Challenges We Overcame

ğŸ§© Integrating three AI systems (vision + language + voice)<br>
ğŸ¤ Managing latency in voice-triggered queries<br>
ğŸ¦¯ Translating object positions into spatial guidance<br>
ğŸ§ Designing a calm and empathetic voice UX<br>

---

## ğŸŒº Accomplishments

âœ… End-to-end multimodal pipeline: *Detection â†’ Scene Summary â†’ Speech Output*<br>
âœ… Scene-aware conversational Gemini responses<br>
âœ… Periodic voice prompts (every 10 seconds)<br>
âœ… Inclusive voice-first interface tested with real users<br>

---

## ğŸ“š What We Learned

* **Context > Detection:** Users need **actionable guidance**, not raw data
* **Voice-first Design** improves trust and usability
* **Multimodal AI** bridges the gap between accessibility and autonomy
* Accessibility = intuitive speech, minimal friction, and reliability

---

## ğŸš€ Next Steps

ğŸ¦¡ Integrate **haptic belt feedback** for spatial direction<br>
ğŸ—ºï¸ Add **indoor navigation** using AR markers<br>
ğŸ“± Launch a **mobile app** (Flutter) with offline support<br>
ğŸ§  Expand **Gemini context memory** for multi-turn conversations<br>

---

## â¤ï¸ Why Spidey Sense

**Spidey Sense** empowers visually impaired users to move confidently and independently â€” combining **sight, speech, and spatial intelligence** into one assistive companion.
Itâ€™s more than an app â€” itâ€™s **AI that helps you *feel* your surroundings.**
